{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas               as pd\n",
    "import matplotlib.pyplot    as plt\n",
    "import numpy                as np\n",
    "import glob\n",
    "import random\n",
    "\n",
    "training_image_path = 'datasets/training/'\n",
    "training_tnf_csv    = 'training_data/tps'\n",
    "\n",
    "train_pd = pd.read_csv(training_tnf_csv + '/train.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_pd = pd.DataFrame(columns=['Parameter','Min','Max'])\n",
    "\n",
    "# Find the min & max values of each parameter\n",
    "for i in range(1,73):\n",
    "    name = 't' + str(i)\n",
    "    data = train_pd[name]\n",
    "    plt.scatter(np.linspace(0, 1, len(data)), data)\n",
    "    \n",
    "    transforms_pd = transforms_pd.append({\n",
    "        'Parameter': name, \n",
    "        'Min':       np.amin(data), \n",
    "        'Max':       np.amax(data)    \n",
    "        }, ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_transform(pd='', image_name='', min_max=[], name=True):\n",
    "    \"\"\"\n",
    "    Create a random transform by sampling from each parameter. \n",
    "    Assume a uniform distribution between min & max values.\n",
    "    \"\"\"\n",
    "    \n",
    "    output = {}\n",
    "    if name:\n",
    "        output['ImageA'] = image_name\n",
    "        output['ImageB'] = image_name\n",
    "        \n",
    "    if len(min_max):\n",
    "        for i in range(1,19):\n",
    "            name    = 't' + str(i)\n",
    "            value   = np.random.uniform(min_max[0,i-1], min_max[1,i-1])\n",
    "            output[name] = [value]\n",
    "            \n",
    "    else:\n",
    "        for i in range(1,73):\n",
    "            name    = 't' + str(i)\n",
    "            row     = pd[pd['Parameter']==name]\n",
    "            value   = np.random.uniform(row['Min'], row['Max'])[0]\n",
    "            output[name] = [value]\n",
    "            \n",
    "    \n",
    "        \n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add MRI images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['HMU_003_DB', 'HMU_011_MQ', 'HMU_025_SH']\n",
    "\n",
    "in_vivo = {\n",
    "    'HMU_003_DB': [20,19,12],\n",
    "    'HMU_007_TN': [19,18,16],\n",
    "    'HMU_010_FH' : [19,17,13,12],\n",
    "    'HMU_011_MQ': [18,12,10,9],\n",
    "    'HMU_025_SH': [22,19,15]\n",
    "}\n",
    "\n",
    "updated_train_pd = train_pd.copy()\n",
    "number_samples   = 0\n",
    "\n",
    "for name in file_names:\n",
    "    # MRI \n",
    "    mri_slices = in_vivo[name]\n",
    "    for slice in mri_slices:\n",
    "        image_name  = 'mri_' + name + '_' + str(slice) + '.png'\n",
    "        new_row     = random_transform(transforms_pd, image_name)\n",
    "        \n",
    "        new_row_pd       = pd.DataFrame.from_dict(new_row)\n",
    "        updated_train_pd = pd.concat((updated_train_pd, new_row_pd), ignore_index=True)\n",
    "        \n",
    "        number_samples += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add histology images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = ['HMU_003_DB', 'HMU_011_MQ', 'HMU_025_SH','HMU_056_JH','HMU_060_CH','HMU_063_RS','HMU_064_SB','HMU_065_RH','HMU_067_MS']\n",
    "\n",
    "updated_train_pd = train_pd.copy()\n",
    "number_samples   = 0\n",
    "\n",
    "histo = {\n",
    "    'HMU_003_DB': ['_A1', '_A2','_A3','_A5'],\n",
    "    'HMU_011_MQ': ['_4','_5','_6','_7','_8','_9'],\n",
    "    'HMU_025_SH': ['A1','A3','A4','A5'],\n",
    "    'HMU_056_JH': ['A1','A2','A3','A4','A5','A6'], \n",
    "    'HMU_060_CH': ['A1','A2','A3','A4','A5','A6'], \n",
    "    'HMU_063_RS': ['A1','A2','A3','A4','A5'], \n",
    "    'HMU_064_SB': ['A1','A4','A6'], \n",
    "    'HMU_065_RH': ['A1','A2','A3','A4','A5','A6'], \n",
    "    'HMU_067_MS': ['A1','A2','A3','A4','A5','A6','A7']\n",
    "}\n",
    "\n",
    "for i in range(10):\n",
    "    for name in file_names: \n",
    "        histo_slices = histo[name]\n",
    "        for slice in histo_slices:\n",
    "            image_name  = name + str(slice) + '_segmented.png'\n",
    "            new_row     = random_transform(transforms_pd, image_name)\n",
    "            \n",
    "            new_row_pd       = pd.DataFrame.from_dict(new_row)\n",
    "            updated_train_pd = pd.concat((updated_train_pd, new_row_pd), ignore_index=True)\n",
    "            \n",
    "            number_samples += 1\n",
    "\n",
    "print(number_samples)\n",
    "updated_train_pd\n",
    "\n",
    "updated_train_pd.to_csv('training_data/tps/train_updated_47.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrate TPS transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_img import * \n",
    "from geotnf.transformation import GeometricTnf\n",
    "from skimage import io\n",
    "\n",
    "\n",
    "def tps_transform_grid(transforms_pd):\n",
    "    \"\"\"\n",
    "    Apply a random TPS transform to the sample grid\n",
    "    \"\"\"\n",
    "    \n",
    "    transform = random_transform(transforms_pd, name=False)\n",
    "    theta_tps = [transform[i] for i in transform]\n",
    "    theta_tps = torch.Tensor(np.transpose(theta_tps))\n",
    "\n",
    "    # Preprocess image \n",
    "    source_image = io.imread('../Dataset/Data/grid.png')\n",
    "    source_image = np.squeeze(source_image[:,:,:3])\n",
    "    source_image[source_image<250] = 0\n",
    "    source_image = process_image(source_image,use_cuda=False, out_size=1024)\n",
    "\n",
    "    # TPS transformation\n",
    "    tpsTnf       = GeometricTnf(geometric_model='tps', out_h=400, out_w=400, use_cuda=False)\n",
    "    warped_image = tpsTnf(source_image,theta_tps)\n",
    "        \n",
    "    # Un-normalize images and convert to numpy\n",
    "    warped_image_np = normalize_image(warped_image,forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()\n",
    "        \n",
    "    # Ignore negative values\n",
    "    warped_image_np[warped_image_np < 0] = 0    \n",
    "    source_image = source_image.permute(0,2,3,1)\n",
    "    return source_image, warped_image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_image, warped_image = tps_transform_grid(transforms_pd)\n",
    "\n",
    "fig, axs = plt.subplots(1,2)\n",
    "axs[0].imshow(np.squeeze(source_image))\n",
    "axs[1].imshow(warped_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process_img import * \n",
    "from geotnf.transformation import GeometricTnf\n",
    "from skimage import io\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "def tps_Grid(theta_tps, norm=False):\n",
    "    \"\"\"\n",
    "    Apply a random TPS transform to the sample grid\n",
    "    \"\"\"\n",
    "    \n",
    "    theta_tps = torch.Tensor(theta_tps)\n",
    "    if norm:\n",
    "        theta_tps = normalize(theta_tps)\n",
    "\n",
    "    # Preprocess image \n",
    "    source_image = io.imread('../Dataset/Data/grid.png')\n",
    "    source_image = np.squeeze(source_image[:,:,:3])\n",
    "    source_image[source_image<250] = 0\n",
    "    source_image = process_image(source_image, use_cuda=False, out_size=1024)\n",
    "\n",
    "    # TPS transformation\n",
    "    tpsTnf_mri   = GeometricTnf(geometric_model='tps-mri', out_h=400, out_w=400, use_cuda=False)\n",
    "    warped_image = tpsTnf_mri(source_image,theta_tps)\n",
    "        \n",
    "    # Un-normalize images and convert to numpy\n",
    "    warped_image_np = normalize_image(warped_image,forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()\n",
    "        \n",
    "    # Ignore negative values\n",
    "    warped_image_np[warped_image_np < 0] = 0    \n",
    "    source_image = source_image.permute(0,2,3,1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    axs[0].imshow(np.squeeze(source_image))\n",
    "    axs[1].imshow(warped_image_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tps =[\n",
    "            [\n",
    "                -0.6362779140472412,\n",
    "                -1.6981632709503174,\n",
    "                0.587742805480957,\n",
    "                0.8616968393325806,\n",
    "                -2.4061849117279053,\n",
    "                -4.045632839202881,\n",
    "                -0.05718374252319336,\n",
    "                1.5738213062286377,\n",
    "                1.9545316696166992,\n",
    "                -2.3466782569885254,\n",
    "                1.491770625114441,\n",
    "                1.8796687126159668,\n",
    "                -1.5033074617385864,\n",
    "                -2.048987865447998,\n",
    "                2.108168601989746,\n",
    "                0.7781451940536499,\n",
    "                -0.4112209379673004,\n",
    "                1.1588855981826782\n",
    "            ]\n",
    "        ]\n",
    "tps_test =[[-1.0,-1.0,-1.0,0.0,0.0,0.0,1.0,1.0,1.0,-1.0,0.0,1.0,-1.0,0.0,1.0,-1.0,0.0,1.0]]\n",
    "\n",
    "tps_Grid(tps_test)\n",
    "tps_Grid(tps)\n",
    "tps_Grid(tps, norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = [[-1.1, -1.1, -1.1, -0.1, -0.1, -0.1, 0.9, 0.9, 0.9,-1.1, -0.1, 0.9,-1.1, -0.1, 0.9,-1.1, -0.1, 0.9],\n",
    "            [-0.9, -0.9, -0.9, 0.1,  0.1,  0.1, 1.1, 1.1, 1.1,-0.9, 0.1, 1.1,-0.9, 0.1, 1.1,-0.9, 0.1, 1.1]]\n",
    "min_max = np.array(min_max)\n",
    "\n",
    "tps = []\n",
    "for i in range(18):\n",
    "    tps.append(np.random.uniform(min_max[0,i], min_max[1,i]))\n",
    "tps = [tps]\n",
    "\n",
    "tps_Grid(tps)\n",
    "tps_Grid(tps, norm=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRI training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['ImageA','ImageB']\n",
    "for i in range(1,73):\n",
    "    columns.append('t' + str(i))\n",
    "    \n",
    "train_mri_pd = pd.DataFrame(columns=columns)\n",
    "test_mri_pd  = pd.DataFrame(columns=columns)\n",
    "\n",
    "number_samples   = 0\n",
    "\n",
    "file_names =  [i[20:] for i in sorted(glob.glob('./datasets/training/mri_*'))]\n",
    "\n",
    "for i in range(10):\n",
    "    count = 0\n",
    "    for name in file_names: \n",
    "        new_row     = random_transform(transforms_pd, name)\n",
    "        new_row_pd  = pd.DataFrame.from_dict(new_row)\n",
    "        \n",
    "        if count < 61:\n",
    "            train_mri_pd = pd.concat((train_mri_pd, new_row_pd), ignore_index=True)\n",
    "        else:\n",
    "            test_mri_pd  = pd.concat((test_mri_pd, new_row_pd), ignore_index=True)\n",
    "    \n",
    "        number_samples += 1\n",
    "        count          += 1\n",
    "\n",
    "print(number_samples)\n",
    "train_mri_pd.to_csv('training_data/tps/mri_train.csv', index=False)\n",
    "test_mri_pd.to_csv('training_data/tps/mri_test.csv', index=False)\n",
    "test_mri_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max = [[-1.1, -1.1, -1.1, -0.1, -0.1, -0.1, 0.9, 0.9, 0.9,-1.1, -0.1, 0.9,-1.1, -0.1, 0.9,-1.1, -0.1, 0.9],\n",
    "            [-0.9, -0.9, -0.9, 0.1,  0.1,  0.1, 1.1, 1.1, 1.1,-0.9, 0.1, 1.1,-0.9, 0.1, 1.1,-0.9, 0.1, 1.1]]\n",
    "\n",
    "min_max = np.array(min_max)\n",
    "\n",
    "columns = ['ImageA','ImageB']\n",
    "for i in range(1,18):\n",
    "    columns.append('t' + str(i))\n",
    "    \n",
    "train_mri_pd = pd.DataFrame(columns=columns)\n",
    "test_mri_pd  = pd.DataFrame(columns=columns)\n",
    "\n",
    "number_samples   = 0\n",
    "\n",
    "file_names =  [i[20:] for i in sorted(glob.glob('./datasets/training/mri_*'))]\n",
    "\n",
    "for i in range(10):\n",
    "    count = 0\n",
    "    for name in file_names: \n",
    "        new_row     = random_transform(image_name=name, min_max=min_max)\n",
    "        new_row_pd  = pd.DataFrame.from_dict(new_row)\n",
    "        \n",
    "        if count < 61:\n",
    "            train_mri_pd = pd.concat((train_mri_pd, new_row_pd), ignore_index=True)\n",
    "        else:\n",
    "            test_mri_pd  = pd.concat((test_mri_pd, new_row_pd), ignore_index=True)\n",
    "    \n",
    "        number_samples += 1\n",
    "        count          += 1\n",
    "\n",
    "print(number_samples)\n",
    "train_mri_pd.to_csv('training_data/tps/mri_train.csv', index=False)\n",
    "test_mri_pd.to_csv('training_data/tps/mri_test.csv', index=False)\n",
    "test_mri_pd\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create histo + mri csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_from_files(file_names, transforms_pd, train_pd, number_samples):\n",
    "    count = 0\n",
    "    \n",
    "    for name in file_names: \n",
    "        new_row     = random_transform(transforms_pd, name)\n",
    "        new_row_pd  = pd.DataFrame.from_dict(new_row)\n",
    "        \n",
    "        train_pd    = pd.concat((train_pd, new_row_pd), ignore_index=True)\n",
    "        count       += 1\n",
    "        \n",
    "    #print('Count: ', count)\n",
    "    number_samples += count\n",
    "    \n",
    "    return train_pd, number_samples\n",
    "        \n",
    "        \n",
    "columns = ['ImageA','ImageB']\n",
    "for i in range(1,73):\n",
    "    columns.append('t' + str(i))\n",
    "\n",
    "train_pd    = pd.DataFrame(columns=columns)\n",
    "\n",
    "hist_names1    = [i[20:] for i in (glob.glob('./datasets/training/hist*'))]\n",
    "hist_names2    = [i[20:] for i in (glob.glob('./datasets/training/HMU*'))]\n",
    "mri_names1     = [i[20:] for i in (glob.glob('./datasets/training/mri_T*'))]\n",
    "mri_names2     = [i[20:] for i in (glob.glob('./datasets/training/mri_H*'))]\n",
    "dwi_names      = [i[20:] for i in (glob.glob('./datasets/training/dwi*'))]\n",
    "b90_names      = [i[20:] for i in (glob.glob('./datasets/training/b90*'))]\n",
    "\n",
    "print(len(hist_names1),len(hist_names2),len(mri_names1),len(mri_names2),len(dwi_names),len(b90_names))\n",
    "\n",
    "number_samples   = 0\n",
    "for i in range(10):\n",
    "    random.shuffle(mri_names1)\n",
    "    random.shuffle(mri_names2)\n",
    "    \n",
    "    train_pd, number_samples = add_from_files(hist_names1,      transforms_pd, train_pd, number_samples)\n",
    "    train_pd, number_samples = add_from_files(hist_names2,      transforms_pd, train_pd, number_samples)\n",
    "    train_pd, number_samples = add_from_files(mri_names1[:30],  transforms_pd, train_pd, number_samples)\n",
    "    train_pd, number_samples = add_from_files(mri_names2[:30],  transforms_pd, train_pd, number_samples)\n",
    "    train_pd, number_samples = add_from_files(dwi_names,        transforms_pd, train_pd, number_samples)\n",
    "    train_pd, number_samples = add_from_files(b90_names,        transforms_pd, train_pd, number_samples)\n",
    "\n",
    "print(number_samples)\n",
    "train_pd.to_csv('training_data/tps/histo_mri_dwi_train.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output transforms: histo-t2-dwi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot    as plt\n",
    "import numpy                as np\n",
    "from process_img import * \n",
    "from geotnf.transformation import GeometricTnf\n",
    "from skimage import io\n",
    "\n",
    "def warp_image(source_image, transforms_1, transforms_2, affTnf, tpsTnf, tpsMRITnf):\n",
    "    # Transforms\n",
    "    theta_aff_1, theta_aff_2, theta_tps_1 = transforms_1\n",
    "    theta_aff_3, theta_aff_4, theta_tps_2 = transforms_2\n",
    "    \n",
    "    # Warped with transform 1\n",
    "    warped_image_1 = affTnf(source_image,     theta_aff_1.view(-1,2,3))\n",
    "    warped_image_1 = affTnf(warped_image_1,   theta_aff_2.view(-1,2,3))\n",
    "    warped_image_1 = tpsTnf(warped_image_1,   theta_tps_1)\n",
    "    \n",
    "    # Warped with transform 2\n",
    "    warped_image_2 = affTnf(source_image,       theta_aff_3.view(-1,2,3))\n",
    "    warped_image_2 = affTnf(warped_image_2,     theta_aff_4.view(-1,2,3))\n",
    "    warped_image_2 = tpsMRITnf(warped_image_2,  theta_tps_2)\n",
    "    \n",
    "    # Warped with transform 1+2\n",
    "    warped_image_total = affTnf(warped_image_1,         theta_aff_3.view(-1,2,3))\n",
    "    warped_image_total = affTnf(warped_image_total,     theta_aff_4.view(-1,2,3))\n",
    "    warped_image_total = tpsMRITnf(warped_image_total,  theta_tps_2)\n",
    "    \n",
    "    # Un-normalize image and convert to numpy\n",
    "    warped_image_1_np = normalize_image(warped_image_1,forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()\n",
    "    warped_image_2_np = normalize_image(warped_image_2,forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()\n",
    "    warped_image_total_np = normalize_image(warped_image_total,forward=False).data.squeeze(0).transpose(0,1).transpose(1,2).cpu().numpy()\n",
    "        \n",
    "    # Ignore negative values\n",
    "    warped_image_1_np[warped_image_1_np < 0] = 0\n",
    "    warped_image_2_np[warped_image_2_np < 0] = 0    \n",
    "    warped_image_total_np[warped_image_total_np < 0] = 0 \n",
    "    \n",
    "    return warped_image_1_np, warped_image_2_np, warped_image_total_np\n",
    "    \n",
    "    \n",
    "def tps_Grid_full(transforms_1, transforms_2, histo_image, out_size=500, use_cuda=False):\n",
    "    \"\"\"\n",
    "    Apply transforms to the sample grid\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load geometric models\n",
    "    affTnf      = GeometricTnf(geometric_model='affine', out_h=out_size, out_w=out_size, use_cuda=use_cuda)\n",
    "    tpsTnf      = GeometricTnf(geometric_model='tps',    out_h=out_size, out_w=out_size, use_cuda=use_cuda)\n",
    "    tpsMRITnf   = GeometricTnf(geometric_model='tps-mri',out_h=out_size, out_w=out_size, use_cuda=use_cuda)\n",
    "    \n",
    "    # Preprocess image \n",
    "    grid_image = io.imread('../Dataset/Data/grid.png')\n",
    "    grid_image = np.squeeze(grid_image[:,:,:3])\n",
    "    grid_image[grid_image<250] = 0\n",
    "    \n",
    "    grid_image  = process_image(grid_image, use_cuda=False, high_res=True)\n",
    "    histo_image = process_image(histo_image,  use_cuda=False, high_res=True)\n",
    "\n",
    "    grid_warped_1,  grid_warped_2,  grid_warped_total  = warp_image(grid_image,  transforms_1, transforms_2, affTnf, tpsTnf, tpsMRITnf)\n",
    "    histo_warped_1, histo_warped_2, histo_warped_total = warp_image(histo_image, transforms_1, transforms_2, affTnf, tpsTnf, tpsMRITnf)\n",
    "\n",
    "    grid_image  = grid_image.permute(0,2,3,1)\n",
    "    histo_image = histo_image.permute(0,2,3,1)\n",
    "    \n",
    "    fig, axs = plt.subplots(2,4, figsize=(15, 6))\n",
    "    axs[0,0].set_title('Source')\n",
    "    axs[0,0].imshow(np.squeeze(grid_image))\n",
    "    axs[1,0].imshow(np.squeeze(histo_image))\n",
    "    \n",
    "    axs[0,1].set_title('Transform histo-T2')\n",
    "    axs[0,1].imshow(grid_warped_1)\n",
    "    axs[1,1].imshow(histo_warped_1)\n",
    "    \n",
    "    axs[0,2].set_title('Transform T2-DWI')\n",
    "    axs[0,2].imshow(grid_warped_2)\n",
    "    axs[1,2].imshow(histo_warped_2)\n",
    "    \n",
    "    axs[0,3].set_title('Transform histo-DWI')\n",
    "    axs[0,3].imshow(grid_warped_total)\n",
    "    axs[1,3].imshow(histo_warped_total)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transform_histo_dwi import get_data, create_tensor\n",
    "from glob import glob\n",
    "\n",
    "sid = 'HMU_010_FH'\n",
    "json_path_histo_t2  = './transforms/transform_' + sid + '.json'\n",
    "json_path_t2_dwi    = './transforms/transform_' + sid + '_T2_DWI.json'\n",
    "img_path   = os.path.join('./results/preprocess/hist' , sid+'_high_res/')\n",
    "\n",
    "json_data_histo_t2 = get_data(json_path_histo_t2)\n",
    "json_data_t2_dwi   = get_data(json_path_t2_dwi)\n",
    "\n",
    "# Get image paths\n",
    "img_path   = os.path.join('./results/preprocess/hist' , sid)\n",
    "all_paths = sorted(glob(img_path + '/hist*.png'))\n",
    "\n",
    "# Get transformation params\n",
    "for i in range(4):\n",
    "    theta_aff_1 = json_data_histo_t2[str(i)][\"affine_1\"]\n",
    "    theta_aff_2 = json_data_histo_t2[str(i)][\"affine_2\"]\n",
    "    theta_tps_1 = json_data_histo_t2[str(i)][\"tps\"]\n",
    "    transforms_1  = (create_tensor(theta_aff_1, use_cuda=False), create_tensor(theta_aff_2, use_cuda=False), create_tensor(theta_tps_1, use_cuda=False)) \n",
    "\n",
    "    theta_aff_3 = json_data_t2_dwi[str(i)][\"affine_1\"]\n",
    "    theta_aff_4 = json_data_t2_dwi[str(i)][\"affine_2\"]\n",
    "    theta_tps_2 = json_data_t2_dwi[str(i)][\"tps\"]\n",
    "    transforms_2  = (create_tensor(theta_aff_3, use_cuda=False), create_tensor(theta_aff_4, use_cuda=False), create_tensor(theta_tps_2, use_cuda=False))\n",
    "    \n",
    "    source_img = io.imread(all_paths[i])\n",
    "\n",
    "    tps_Grid_full(transforms_1, transforms_2, source_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
